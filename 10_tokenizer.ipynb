{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "> We are going to tokenize using 2 different strategies. The first one is using stopwords (read the main README for more information). And the second one, is a nltk grammar regexp parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from keywords2vec.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "NUMBERS_STOPWORDS = {\n",
    "    \"en\": [\n",
    "        \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"twenty-one\", \"twenty-two\", \"twenty-three\", \"twenty-four\", \"twenty-five\", \"twenty-six\", \"twenty-seven\", \"twenty-eight\", \"twenty-nine\", \"thirty\", \"thirty-one\", \"thirty-two\", \"thirty-three\", \"thirty-four\", \"thirty-five\", \"thirty-six\", \"thirty-seven\", \"thirty-eight\", \"thirty-nine\", \"forty\", \"forty-one\", \"forty-two\", \"forty-three\", \"forty-four\", \"forty-five\", \"forty-six\", \"forty-seven\", \"forty-eight\", \"forty-nine\", \"fifty\", \"fifty-one\", \"fifty-two\", \"fifty-three\", \"fifty-four\", \"fifty-five\", \"fifty-six\", \"fifty-seven\", \"fifty-eight\", \"fifty-nine\", \"sixty\", \"sixty-one\", \"sixty-two\", \"sixty-three\", \"sixty-four\", \"sixty-five\", \"sixty-six\", \"sixty-seven\", \"sixty-eight\", \"sixty-nine\", \"seventy\", \"seventy-one\", \"seventy-two\", \"seventy-three\", \"seventy-four\", \"seventy-five\", \"seventy-six\", \"seventy-seven\", \"seventy-eight\", \"seventy-nine\", \"eighty\", \"eighty-one\", \"eighty-two\", \"eighty-three\", \"eighty-four\", \"eighty-five\", \"eighty-six\", \"eighty-seven\", \"eighty-eight\", \"eighty-nine\", \"ninety\", \"ninety-one\", \"ninety-two\", \"ninety-three\", \"ninety-four\", \"ninety-five\", \"ninety-six\", \"ninety-seven\", \"ninety-eight\", \"ninety-nine\"\n",
    "    ],\n",
    "    \"es\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_stopwords(stopwords=None, additional_stopwords=None, lang=\"en\"):\n",
    "    if stopwords is None:\n",
    "        stopwords = safe_get_stop_words(lang) + (NUMBERS_STOPWORDS.get(lang) or [])\n",
    "    if additional_stopwords:\n",
    "        stopwords += additional_stopwords\n",
    "    return [\n",
    "        stopword\n",
    "        for stopword in stopwords\n",
    "        if stopword\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_one(text, stopwords=None, additional_stopwords=None, lang=\"en\", split_by_stopwords=True):\n",
    "    stopwords = prepare_stopwords(stopwords, additional_stopwords, lang)\n",
    "    text_part = text.lower()\n",
    "\n",
    "    regexs = []\n",
    "    if split_by_stopwords:\n",
    "        # Remove all stopwords by a !, we are searching for the stopword (bounded)\n",
    "        regexs.append(\n",
    "            (\"\\\\b\" + \"\\\\b|\\\\b\".join(stopwords), \"!!\")\n",
    "        )\n",
    "    # Must be executed in order\n",
    "    regexs += [\n",
    "        (\"â€™\", \"'\"),\n",
    "        # Remove all non alpha, numeric, spaces, - or single quote\n",
    "        (r'([^a-z0-9\\u00C0-\\u1FFF\\u2C00-\\uD7FF \\n\\-\\'])', \"!!\"),\n",
    "        # remove only words numbers\n",
    "        (r'(^|[ !])[\\-0-9]+([ !]|$)', \"!!\"),\n",
    "        # remove hyphen-minus for keywords starting or ending with it\n",
    "        (r'((^|[ !])[\\-\\']+)|([\\-\\']+([ !]|$))', \"!!\"),\n",
    "        # remove spaces between !\n",
    "        (r' *! *', \"!!\"),\n",
    "        # generate multiple ! need for next regex\n",
    "        (r'!', \"!!\"),\n",
    "        # remove one character keyword\n",
    "        (r'(^|!)[^!\\n](!|$)', \"!!\"),\n",
    "        # remove multiple ! (!!!!)\n",
    "        (r'!+', \"!\"),\n",
    "        # remove first and last !\n",
    "        (r'(^!+)|(!+$)', \"\")\n",
    "    ]\n",
    "\n",
    "    for regex, replacement in regexs:\n",
    "        text_part = re.sub(regex, replacement, text_part, flags=re.M)\n",
    "    return text_part\n",
    "\n",
    "\n",
    "# Second option to tokenize the information\n",
    "def get_nodes_for_ntlk(parent, stopwords, valid_labels):\n",
    "    keywords = []\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() in valid_labels:\n",
    "                phrase = \" \".join([key.lower() for key, value in node.leaves()])\n",
    "                phrase = unidecode.unidecode(phrase)\n",
    "                for subtree in node.subtrees():\n",
    "                    subtree_keywords = get_nodes_for_ntlk(subtree, stopwords, valid_labels)\n",
    "                    keywords.extend(subtree_keywords)\n",
    "                if phrase not in stopwords:\n",
    "                    pattern = re.compile(r\"([^\\s\\w-]|_)+\")\n",
    "                    phrase = pattern.sub('', phrase).strip()\n",
    "                    keywords.append(phrase)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def tokenize_by_nltk(text, stopwords=None, additional_stopwords=None, lang=\"en\"):\n",
    "    stopwords = prepare_stopwords(stopwords, additional_stopwords, lang)\n",
    "    # grammar = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "    grammar = r\"\"\"\n",
    "        PHRASE1: {<NN.*>+ <DT>*}\n",
    "        PHRASE2: {<JJ> <PHRASE1>+}\n",
    "        PHRASE3: {<JJ>+ <PHRASE2>}\n",
    "        PHRASE4: {(<JJ>* <NN.*>+ <IN>) <PHRASE3>}\n",
    "    \"\"\"\n",
    "    valid_labels = [\"PHRASE1\", \"PHRASE2\", \"PHRASE3\", \"PHRASE4\", \"KT\"]\n",
    "\n",
    "    chunker = nltk.RegexpParser(grammar, loop=5)\n",
    "    chunker2 = nltk.RegexpParser(r\"KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}\", loop=5)\n",
    "    output = \"\"\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        sentences = nltk.sent_tokenize(line)\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "        keyphrases = []\n",
    "        relevant_words = []\n",
    "        for sentence in sentences:\n",
    "            keyphrases.append(chunker.parse(sentence))\n",
    "            keyphrases.append(chunker2.parse(sentence))\n",
    "        for elem in keyphrases:\n",
    "            relevant_words += get_nodes_for_ntlk(elem, stopwords, valid_labels)\n",
    "        output += \"!\".join(relevant_words) + \"!\"\n",
    "\n",
    "    #output = re.sub(\"\\\\b\" + \"\\\\b|!\".join(stopwords), \"!\", output, flags=re.M).lower()\n",
    "    output = tokenize_one(output, split_by_stopwords=False)\n",
    "    return output\n",
    "\n",
    "\n",
    "def tokenize(text, text_output=False, lang=\"en\", keywords_w_stopwords=False, merge=True):\n",
    "    outputs = []\n",
    "    tokenizers = [tokenize_one]\n",
    "    if lang == \"en\" and keywords_w_stopwords:\n",
    "        tokenizers.append(tokenize_by_nltk)\n",
    "\n",
    "    for tokenizer_el in tokenizers:\n",
    "        outputs.append(\n",
    "            tokenizer_el(\n",
    "                text,\n",
    "                lang=lang\n",
    "            )\n",
    "        )\n",
    "    if text_output:\n",
    "        if merge:\n",
    "            return \"!\".join(outputs)\n",
    "        else:\n",
    "            return outputs\n",
    "    keywords = [\n",
    "        [\n",
    "            keyword.strip()\n",
    "            for phrase in re.split(\"\\r\\n|\\n\", output)\n",
    "            for keyword in phrase.split(\"!\")\n",
    "        ]\n",
    "        for output in outputs\n",
    "    ]\n",
    "    if merge:\n",
    "        return [item for sublist in keywords for item in sublist]\n",
    "    else:\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"tokenize\" class=\"doc_header\"><code>tokenize</code><a href=\"__main__.py#L101\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>tokenize</code>(**`text`**, **`text_output`**=*`False`*, **`lang`**=*`'en'`*, **`keywords_w_stopwords`**=*`False`*, **`merge`**=*`True`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_wikipedia\n",
    "example_text = \"\"\"The modern sovereign state of Chile is among South America's most economically and socially stable and prosperous nations, with a high-income economy and high living standards.[11][12] It leads Latin American nations in rankings of human development, competitiveness, income per capita, globalization, state of peace, economic freedom, and low perception of corruption.[13] It also ranks high regionally in sustainability of the state, and democratic development.[14] Currently it also has the lowest homicide rate in the Americas after Canada. Chile is a founding member of the United Nations, the Union of South American Nations (UNASUR), the Community of Latin American and Caribbean States (CELAC) and the Pacific Alliance, and joined the Organisation for Economic Co-operation and Development (OECD) in 2010.\"\"\"\n",
    "tokenized_text = tokenize(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the text, by default use two methods and merge them, so you might see duplicated keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern sovereign state',\n",
       " 'chile',\n",
       " \"among south america's\",\n",
       " 'economically',\n",
       " 'socially stable',\n",
       " 'prosperous nations',\n",
       " 'high-income economy',\n",
       " 'high living standards',\n",
       " 'leads latin american nations',\n",
       " 'rankings',\n",
       " 'human development',\n",
       " 'competitiveness',\n",
       " 'income per capita',\n",
       " 'globalization',\n",
       " 'state']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to how see the result from each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenize(example_text, merge=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized using only stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern sovereign state',\n",
       " 'chile',\n",
       " \"among south america's\",\n",
       " 'economically',\n",
       " 'socially stable',\n",
       " 'prosperous nations',\n",
       " 'high-income economy',\n",
       " 'high living standards',\n",
       " 'leads latin american nations',\n",
       " 'rankings',\n",
       " 'human development',\n",
       " 'competitiveness',\n",
       " 'income per capita',\n",
       " 'globalization',\n",
       " 'state']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized using only nltk, this method complement to find keywords with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sovereign state',\n",
       " 'modern sovereign state',\n",
       " 'chile',\n",
       " 'south america',\n",
       " 'nations',\n",
       " 'prosperous nations',\n",
       " 'economy',\n",
       " 'high-income economy',\n",
       " 'living standards',\n",
       " 'high living standards',\n",
       " 'modern sovereign state of chile',\n",
       " 'south america',\n",
       " 'prosperous nations',\n",
       " 'high-income economy',\n",
       " 'high living standards']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[1][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use them with the default options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and return plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"modern sovereign state!chile!among south america's!economically!socially stable!prosperous nations!high-income economy!high living standards!leads latin american nations!rankings!human development!competitiveness!income per capita!globalization!state!peace!economic freedom!low perception!corruption!also ranks high regionally!sustainability!state!democratic development!currently!also!lowest homicide rate!americas!canada!chile!founding member!united nations!union!south american nations!unasur!community!latin american!caribbean states!celac!pacific alliance!joined!organisation!economic co-operation!development!oecd!sovereign state!modern sovereign state!chile!south america!nations!prosperous nations!economy!high-income economy!living standards!high living standards!modern sovereign state of chile!south america!prosperous nations!high-income economy!high living standards!nations!american nations!nations!latin american nations!rankings!development!human development!competitiveness!income!capita!globalization!state!peace!freedom!economic freedom!perception!low perception!corruption!latin american nations in rankings!human development!competitiveness!income per capita!globalization!state of peace!economic freedom!low perception of corruption!sustainability!state!development!democratic development!sustainability!state!democratic development!currently!homicide rate!americas!canada!currently!homicide rate!americas after canada!chile!member!founding member!united nations!union!south!nations!american nations!unasur!community!states!caribbean states!celac!pacific alliance!organisation!economic co-operation!development!oecd!chile!founding member!united nations!union of south!american nations!unasur!community!caribbean states!celac!pacific alliance!organisation for economic co-operation!development!oecd\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenize(example_text, text_output=True)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
