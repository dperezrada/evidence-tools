{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from keywords2vec.main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keywords2vec\n",
    "\n",
    "> A simple and fast way to generate a word2vec model, with multi-word keywords instead of single words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similar keywords for \"__obesity__\"\n",
    "\n",
    "| index | term                        |\n",
    "|-------|-----------------------------|\n",
    "| 0     | overweight                  |\n",
    "| 1     | obese                       |\n",
    "| 2     | physical inactivity         |\n",
    "| 3     | excess weight               |\n",
    "| 4     | obese adults                |\n",
    "| 5     | high bmi                    |\n",
    "| 6     | obese adults                |\n",
    "| 7     | obese people                |\n",
    "| 8     | obesity-related outcomes    |\n",
    "| 9     | obesity among children      |\n",
    "| 10    | poor sleep quality          |\n",
    "| 11    | ssbs                        |\n",
    "| 12    | obese populations           |\n",
    "| 13    | cardiometabolic risk        |\n",
    "| 14    | abdominal obesity           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install keywords2vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download some example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = \"epistemonikos_data_sample.tsv.gz\"\n",
    "\n",
    "!wget \"https://s3.amazonaws.com/episte-labs/epistemonikos_data_sample.tsv.gz\" -O \"{data_filepath}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model. If you need the vectors, take a look [here](30_main.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: epistemonikos_data_sample.tsv.gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='201' class='' max='201', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [201/201 00:19<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, tree = similars_tree(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get the most similars keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obesity',\n",
       " 'overweight',\n",
       " 'obese',\n",
       " 'physical inactivity',\n",
       " 'excess weight',\n",
       " 'high bmi',\n",
       " 'obese adults',\n",
       " 'obese people',\n",
       " 'obesity-related outcomes',\n",
       " 'obesity among children',\n",
       " 'poor sleep quality',\n",
       " 'ssbs',\n",
       " 'obese populations',\n",
       " 'cardiometabolic risk',\n",
       " 'abdominal obesity']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(tree, labels, \"obesity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heart failure',\n",
       " 'hf',\n",
       " 'chf',\n",
       " 'chronic heart failure',\n",
       " 'reduced ejection fraction',\n",
       " 'unstable angina',\n",
       " 'peripheral vascular disease',\n",
       " 'peripheral arterial disease',\n",
       " 'angina',\n",
       " 'congestive heart failure',\n",
       " 'left ventricular systolic dysfunction',\n",
       " 'acute coronary syndrome',\n",
       " 'heart failure patients',\n",
       " 'acute myocardial infarction',\n",
       " 'left ventricular dysfunction']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(tree, labels, \"heart failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "The idea started in the Epistemonikos database [www.epistemonikos.org](https://www.epistemonikos.org), a database of scientific articles for people making decisions concerning clinical or health-policy questions. In this context the scientific/health language used is complex. You can easily find keywords like:\n",
    "\n",
    " * asthma\n",
    " * heart failure\n",
    " * medial compartment knee osteoarthritis\n",
    " * preserved left ventricular systolic function\n",
    " * non-selective non-steroidal anti-inflammatory drugs\n",
    " \n",
    "We tried some approaches to find those keywords, like ngrams, ngrams + tf-idf, identify entities, among others. But we didn't get really good results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach\n",
    "\n",
    "We found that tokenizing using stopwords + non word characters was really useful for \"finding\" the keywords. An example:\n",
    "\n",
    "* input: \"Timing of replacement therapy for acute renal failure after cardiac surgery\"\n",
    "* output: [\n",
    "\t\"timing\",\n",
    "\t\"replacement therapy\",\n",
    "\t\"acute renal failure\",\n",
    "\t\"cardiac surgery\"\n",
    "]\n",
    "\n",
    "So we basically split the text when we find:\n",
    " * a stopword\n",
    " * a non word character(/,!?. etc) (except from - and ')\n",
    "\n",
    "That's it.\n",
    "\n",
    "But as there were some problem with some keywords that cointain stopwords, like:\n",
    " * Vitamin A\n",
    " * Hepatitis A\n",
    " * Web of Science\n",
    "\n",
    "So we decided to add another method (nltk with some grammar definition) to cover most of the cases. To use this, you need to add the parameter `keywords_w_stopwords=True`, this method is approx 20x slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Seem to be an old idea (2004):\n",
    "\n",
    "*Mihalcea, Rada, and Paul Tarau. \"Textrank: Bringing order into text.\" Proceedings of the 2004 conference on empirical methods in natural language processing. 2004.*\n",
    "\n",
    "Reading an implementation of textrank, I realize they used stopwords to separate and create the graph. Then I though in using it as tokenizer for word2vec\n",
    "\n",
    "As pointed by @deliprao in this [twitter thread](https://twitter.com/jeremyphoward/status/1094025901371621376). It's also used by Rake (2010):\n",
    "\n",
    "*Rose, Stuart & Engel, Dave & Cramer, Nick & Cowley, Wendy. (2010). Automatic Keyword Extraction from Individual Documents. 10.1002/9780470689646.ch1.*\n",
    "\n",
    "As noted by @astent in the Twitter thread, this concept is called chinking (chunking by exclusion)\n",
    "[https://www.nltk.org/book/ch07.html#Chinking](https://www.nltk.org/book/ch07.html#Chinking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-lingual\n",
    "We worked in an implementation, that could be used in multiple languages. Of course not all languages are sutable for using this approach. We have tried with good results in English, Spanish and Portuguese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it online\n",
    "\n",
    "You can try it [here](http://54.196.169.11/episte/) (takes time to load, lowercase only, doesn't work in mobile yet) MPV :)\n",
    "\n",
    "These embedding were created using 827,341 title/abstract from @epistemonikos database.\n",
    "With keywords that repeat at least 10 times. The total vocab is 349,080 keywords (really manageable number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab size\n",
    "\n",
    "One of the main benefit of this method, is the size of the vocabulary. \n",
    "For example, using keywords that repeat at least 10 times, for the Epistemonikos dataset (827,341 title/abstract), we got the following vocab size:\n",
    "\n",
    "| ngrams             | keywords  | comp    |\n",
    "|--------------------|-----------|---------|\n",
    "| 1                  | 127,824   | 36%     |\n",
    "| 1,2                | 1,360,550 | 388%    |\n",
    "| 1-3                | 3,204,099 | 914%    |\n",
    "| 1-4                | 4,461,930 | 1,272%  |\n",
    "| 1-5                | 5,133,619 | 1,464%  |\n",
    "|                    |           |         |\n",
    "| stopword tokenizer | 350,529   | 100%    |\n",
    "\n",
    "More information regarding the comparison, take a look to the folder [analyze](analyze).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been created using [nbdev](https://github.com/fastai/nbdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
